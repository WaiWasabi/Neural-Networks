{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworkFromScratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNy766+mvVRjdn3ZDxs1m8Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WaiWasabi/Neural-Networks/blob/master/NeuralNetworkFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UzuApzmNkz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "  return sigmoid(z)*(1 - sigmoid(z))\n",
        "\n",
        "def cost_derivative(aL, y): # where aL is the vector of output activations and y is the vector of expected outputs.\n",
        "  return aL-y\n",
        "\n",
        "class Network(object):\n",
        "  def __init__(self, sizes):\n",
        "    self.num_layers = len(sizes)\n",
        "    self.sizes = sizes\n",
        "    self.biases = [np.random.randn(x,1) for x in sizes[1:]]\n",
        "    self.weights = [np.random.randn(x,y) for x, y in zip(sizes[1:], sizes[:-1])]\n",
        "\n",
        "  def feedforward(self, a): # mostly used for testing samples.\n",
        "    for w, b in zip(self.weights, self.biases):\n",
        "      a = sigmoid(np.dot(w, a) + b)\n",
        "    return a\n",
        "  \n",
        "  def forward_pass(self, x, y):\n",
        "    activation = x\n",
        "    activations = [x]\n",
        "    zs = []\n",
        "\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      z = np.dot(w, activation) + b\n",
        "      zs.append(z)\n",
        "      activation = sigmoid(z)\n",
        "      activations.append(activation)\n",
        "\n",
        "    return activations, zs, y\n",
        "\n",
        "  def backprop(self, forward_pass):\n",
        "    activations, zs, y = forward_pass\n",
        "    \"\"\"import data from a forward pass\"\"\"\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "    \"\"\"initiate gradients\"\"\"\n",
        "    delta = cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "    nabla_b[-1] = delta\n",
        "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "    for l in range(2, self.num_layers):\n",
        "      z = zs[-l]\n",
        "      delta = np.dot(self.weights[-l+1].transpose(), delta) * sigmoid_prime(z)\n",
        "      nabla_b[-l] = delta\n",
        "      nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "\n",
        "    return(nabla_b, nabla_w)\n",
        "\n",
        "  def update_mini_batch(self, mini_batch, lr): \n",
        "    \"\"\"with mini_batch being a list of tuples (x, y),\n",
        "    where x is a training example and y is expected output,\n",
        "    and lr being the learning rate.\"\"\"\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "    for x, y in mini_batch:\n",
        "      delta_nabla_b, delta_nabla_w = self.backprop(self.forward_pass(x,y))\n",
        "      nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "      nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "    self.biases = [b-(lr/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "    self.weights = [w-(lr/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "  \n",
        "  def SGD(self, train_data, epochs, mini_batch_size, lr, test_data = None):\n",
        "    n = len(train_data)\n",
        "    for i in range(epochs):\n",
        "      random.shuffle(train_data)\n",
        "      mini_batches = [train_data[k : k + mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "      for j in mini_batches:\n",
        "        self.update_mini_batch(j, lr)\n",
        "      #print(f\"Epoch {i} complete\")"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}