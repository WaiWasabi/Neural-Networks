{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mini-batch mlp redo",
      "provenance": [],
      "authorship_tag": "ABX9TyMZfbs7QxwMi3GvQjC1QiVV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WaiWasabi/Neural-Networks/blob/master/Updated%20Multilayer%20Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZC8E7-JQ2CO"
      },
      "source": [
        "***Changelog*** Created 5/29/21\n",
        "\n",
        "---\n",
        "*   **[1.0.0 Alpha]** plan to implement various costs + activation classes, but gradient descent optimizer only.\n",
        "\n",
        "*   **[1.0.1 Alpha]** use of parent classes for activations & losses to inherit from\n",
        "\n",
        "*   **[1.0.2 Alpha]** feedforward and backpropagation are now two separate functions for better readability.\n",
        "\n",
        "*   **[1.0.3 Alpha]** activation & loss are currently passed as class instances to Network, but plan to revert back to the old dictionary method /w string keys.\n",
        "\n",
        "*   **[1.0.0]** Currently functional after correcting incorrect loss derivative function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1xFz6D23Nbj"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqjDsKabaBb8"
      },
      "source": [
        "class DefaultActivation(): # Base class for activations implements sigmoid activation.\n",
        "  def f(self, z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "  \n",
        "  def prime(self, z):\n",
        "    return self.f(z) * (1.0-self.f(z))\n",
        "\n",
        "  def delta(self, z, dLda):\n",
        "    return dLda * self.prime(z)\n",
        "\n",
        "class Sigmoid(DefaultActivation):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUhaMs9YxUri"
      },
      "source": [
        "class DefaultLoss():\n",
        "  def f(self, x, y):\n",
        "    return 0.5*(y-x)**2\n",
        "  \n",
        "  def prime(self, x, y):\n",
        "    return x-y\n",
        "\n",
        "class MSE(DefaultLoss):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AgxM-ZunpI1"
      },
      "source": [
        "def to_one_hot(data, max):\n",
        "  output = []\n",
        "  for index in data:\n",
        "    one_hot = np.zeros((max, 1))\n",
        "    one_hot[index][0] = 1\n",
        "    output.append(one_hot)\n",
        "  return np.array(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekpjyiwr5jWX"
      },
      "source": [
        "class Network():\n",
        "  def __init__(self, sizes, activation, loss):\n",
        "    self.sizes = sizes # array of dimensions for each layer in model\n",
        "    self.num_layers = len(sizes)\n",
        "    self.b = [np.random.standard_normal([i,1]) for i in sizes[1:]] # array of biases for each layer in model\n",
        "    self.w = [np.random.standard_normal([o, i])/np.sqrt(i) for i, o in zip(sizes[:-1], sizes[1:])] # array of weights for each layer in model\n",
        "    self.R = activation\n",
        "    self.L = loss\n",
        "\n",
        "  def summary(self):\n",
        "    print(\"SUMMARY\\n----------------------------\")\n",
        "    print(f\"Number of Layers = {self.num_layers}\")\n",
        "    print(f\"Bias Shapes = {[x.shape for x in self.b]}\")\n",
        "    print(f\"Weight Shapes = {[x.shape for x in self.w]}\")\n",
        "    print(\"----------------------------\")\n",
        "\n",
        "  def feedforward(self, a): # feedforward function for evaluation purposes\n",
        "    for w, b in zip(self.w, self.b):\n",
        "      a = self.R.f(np.matmul(w, a) + b)\n",
        "    return a\n",
        "\n",
        "  def backprop(self, a, y): # calculates gradients of trainable params to be used for gradient descent.\n",
        "    zs, activations = [], [a] # to store z & a values for backprop\n",
        "    for w, b in zip(self.w, self.b):\n",
        "      z = np.matmul(w, a) + b; a = self.R.f(z)\n",
        "      zs.append(z); activations.append(a)\n",
        "    grad_w, grad_b = [None]*len(self.w), [None]*len(self.b)\n",
        "    delta = self.R.delta(zs[-1], self.L.prime(activations[-1], y)) # calculate delta-L (partial derivative of z-L with respect to loss)\n",
        "    grad_w[-1] = np.einsum(\"ijk,ilk->ijl\", delta, activations[-2])\n",
        "    grad_b[-1] = delta\n",
        "    for l in range(2, self.num_layers): # -l represents current layer in backprop\n",
        "      delta = np.einsum(\"jk,ijl,ikl->ikl\", self.w[-l+1], delta, self.R.prime(zs[-l]))\n",
        "      grad_w[-l] = np.einsum(\"ijk,ilk->ijl\", delta, activations[-l-1])\n",
        "      grad_b[-l] = delta\n",
        "    return map(lambda x: np.sum(x, axis=0), grad_w), map(lambda x: np.sum(x, axis=0), grad_b)\n",
        "  \n",
        "  def mini_batch_step(self, mini_batch, lr): # mini_batch data in format (train_features, train_labels)\n",
        "    features, labels = mini_batch\n",
        "    grad_w, grad_b = self.backprop(np.array(features), np.array(labels))\n",
        "    self.w = [w-(lr/len(features))*nw for w, nw in zip(self.w, grad_w)]\n",
        "    self.b = [b-(lr/len(features))*nb for b, nb in zip(self.b, grad_b)]\n",
        "\n",
        "  def SGD(self, dataset, epochs, mini_batch_size, lr, validation_data=None): # dataset data as an iterable of individual (feature, label) pairs\n",
        "    for i in range(epochs):\n",
        "      random.shuffle(dataset)\n",
        "      mini_batches = [zip(*dataset[j:j+mini_batch_size]) for j in range(0, len(dataset), mini_batch_size)]\n",
        "      for mini_batch in mini_batches:\n",
        "        self.mini_batch_step(mini_batch, lr)\n",
        "      print(f\"Epoch {i+1} Complete\")\n",
        "\n",
        "      if validation_data != None:\n",
        "        logistics = self.evaluate(validation_data)\n",
        "        accuracy = round(sum([x == y for x, y in logistics])/len(validation_data)*100, 2)\n",
        "        print(f\"Accuracy: {accuracy} (%)\")\n",
        "\n",
        "  def evaluate(self, test_data): # test_data as an iterable of individual (feature, label pairs)\n",
        "    return [(np.argmax(self.feedforward(feature)), label) for feature, label in test_data]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVpZNBpC4Oxf"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = np.array([x.reshape(-1, 1)/255 for x in x_train])\n",
        "x_test = np.array([x.reshape(-1, 1)/255 for x in x_test])\n",
        "\n",
        "x_train, x_validate = (x_train[0:50000], x_train[50000:60000])\n",
        "y_train, y_validate = (y_train[0:50000], y_train[50000:60000])\n",
        "\n",
        "y_train = to_one_hot(y_train, 10)\n",
        "\n",
        "train_batch = [(x, y) for x, y in zip(x_train, y_train)]\n",
        "test_batch = [(x, y) for x, y in zip(x_test, y_test)]\n",
        "validate_batch = [(x, y) for x, y in zip(x_validate, y_validate)]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxZggEmvFlNA",
        "outputId": "6be63394-cddf-4303-cd2c-dea673dd68f9"
      },
      "source": [
        "x = Network([784, 50, 10], Sigmoid(), MSE())\n",
        "x.summary()\n",
        "x.SGD(train_batch, 10, 30, 3.6, validate_batch)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUMMARY\n",
            "----------------------------\n",
            "Number of Layers = 3\n",
            "Bias Shapes = [(50, 1), (10, 1)]\n",
            "Weight Shapes = [(50, 784), (10, 50)]\n",
            "----------------------------\n",
            "Epoch 1 Complete\n",
            "Accuracy: 93.92 (%)\n",
            "Epoch 2 Complete\n",
            "Accuracy: 95.42 (%)\n",
            "Epoch 3 Complete\n",
            "Accuracy: 95.95 (%)\n",
            "Epoch 4 Complete\n",
            "Accuracy: 96.44 (%)\n",
            "Epoch 5 Complete\n",
            "Accuracy: 96.56 (%)\n",
            "Epoch 6 Complete\n",
            "Accuracy: 96.64 (%)\n",
            "Epoch 7 Complete\n",
            "Accuracy: 96.61 (%)\n",
            "Epoch 8 Complete\n",
            "Accuracy: 96.95 (%)\n",
            "Epoch 9 Complete\n",
            "Accuracy: 96.89 (%)\n",
            "Epoch 10 Complete\n",
            "Accuracy: 96.89 (%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}